{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JXzUvAfZCQwm"
   },
   "source": [
    "# Competition 2 - Image Segmentation\n",
    "The competition 2 is about image segmentation. We need to use Tensorflow 2.0 (with keras) in order to perform semantic segmentation of only 2 different classes inside satellite captures: \"roof\" and \"no roof\". We tried different approaches and we will present the 2 main final ones (the output of this notebook, however, will be not the same we had in the challenge for computer performance reasons).\n",
    "\n",
    "## From Scratch\n",
    "The first part is the code used to model a U-Net based neural network without transfer learning. \n",
    "***\n",
    "The final best result comes from 4 different training sessions on the same model, changing optimizer, learning rate, validation split and other main parameters; this is done in order to let the network to learn in steps, giving it more data at every step and changing the \"speed\" of the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y7Hktfy6Ba9Q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "We are applying data augmentation!\n",
      "\n",
      "\n",
      "WE ARE USING THIS PARAMETERS: lr=2e-05, optimzer=<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x000001F748DEC608>, validation_split=0.05\n",
      "batch_size=64, IOU_Index=0.15\n",
      "Found 7265 images belonging to 1 classes.\n",
      "Found 7265 images belonging to 1 classes.\n",
      "Found 382 images belonging to 1 classes.\n",
      "Found 382 images belonging to 1 classes.\n",
      "Train for 114 steps, validate for 6 steps\n",
      "  1/114 [..............................] - ETA: 2:58"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-aeb910c7ddcd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mVALIDATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m             model.fit(x=train_dataset, epochs=epochs, steps_per_epoch=len(train_img_gen), \n\u001b[1;32m--> 379\u001b[1;33m                       validation_data=valid_dataset, validation_steps=len(valid_img_gen))\n\u001b[0m\u001b[0;32m    380\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_img_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2360\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2362\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2363\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2703\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2705\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2593\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2595\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    976\u001b[0m                                           converted_func)\n\u001b[0;32m    977\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mdistributed_function\u001b[1;34m(input_iterator)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_prepare_feed_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     outputs = strategy.experimental_run_v2(\n\u001b[1;32m---> 85\u001b[1;33m         per_replica_function, args=args)\n\u001b[0m\u001b[0;32m     86\u001b[0m     \u001b[1;31m# Out of PerReplica outputs reduce or pick values to return.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     all_outputs = dist_utils.unwrap_output_dict(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mexperimental_run_v2\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m    761\u001b[0m       fn = autograph.tf_convert(fn, ag_ctx.control_status_ctx(),\n\u001b[0;32m    762\u001b[0m                                 convert_by_default=False)\n\u001b[1;32m--> 763\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    764\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   1817\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1818\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1819\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1821\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   2162\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2163\u001b[0m         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\n\u001b[1;32m-> 2164\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2166\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, x, y, sample_weight, class_weight, reset_metrics, standalone)\u001b[0m\n\u001b[0;32m    431\u001b[0m       \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[0;32m    310\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[0;32m    313\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[1;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[0;32m    267\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backwards\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaled_total_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m           \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaled_total_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m           if isinstance(model.optimizer,\n\u001b[0;32m    271\u001b[0m                         loss_scale_optimizer.LossScaleOptimizer):\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1027\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1030\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m_BiasAddGrad\u001b[1;34m(op, received_grad)\u001b[0m\n\u001b[0;32m    348\u001b[0m   return (received_grad,\n\u001b[0;32m    349\u001b[0m           gen_nn_ops.bias_add_grad(\n\u001b[1;32m--> 350\u001b[1;33m               out_backprop=received_grad, data_format=data_format))\n\u001b[0m\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mbias_add_grad\u001b[1;34m(out_backprop, data_format, name)\u001b[0m\n\u001b[0;32m    761\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0;32m    762\u001b[0m         \u001b[1;34m\"BiasAddGrad\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_backprop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout_backprop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 763\u001b[1;33m                        name=name)\n\u001b[0m\u001b[0;32m    764\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    740\u001b[0m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[0;32m    741\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 742\u001b[1;33m                                  attrs=attr_protos, op_def=op_def)\n\u001b[0m\u001b[0;32m    743\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m     \u001b[1;31m# `outputs` is returned as a separate return value so that the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    593\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0;32m    594\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m         compute_device)\n\u001b[0m\u001b[0;32m    596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3320\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3321\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3322\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3323\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3324\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1784\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[0;32m   1785\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[1;32m-> 1786\u001b[1;33m                                 control_input_ops)\n\u001b[0m\u001b[0;32m   1787\u001b[0m       \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1788\u001b[0m     \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1618\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1619\u001b[1;33m     \u001b[0mc_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1620\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1621\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------IMPORTS AND DIRECTORIES DEFINITIONS\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from IPython.display import FileLink, FileLinks\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import os, random, time, sys, scipy.misc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import regularizers\n",
    "from keras.layers import BatchNormalization as BatchNorm\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Reshape, Activation\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "\n",
    "\n",
    "get_ipython().run_line_magic('matplotlib', 'notebook')\n",
    "\n",
    "dataset_dir = 'Segmentation_Dataset'\n",
    "images_dir = os.path.join(dataset_dir, 'training')\n",
    "training_dir = os.path.join(images_dir, 'images')\n",
    "masks_dir = os.path.join(images_dir, 'masks')\n",
    "test_dir = os.path.join(dataset_dir, 'test/images/img')\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "    \n",
    "# ------------------------------------------------UTILITY FUNCTIONS\n",
    "    \n",
    "def rle_encode(img):\n",
    "    pixels = img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def saveTestDict(test_dir, IMAGE_DIMENSION, treshold):\n",
    "    img_filenames = next(os.walk(test_dir))[2]\n",
    "    \n",
    "    csv_fname = 'results_'\n",
    "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
    "\n",
    "    with open(csv_fname, 'w') as f:\n",
    "        f.write('ImageId,EncodedPixels,Width,Height\\n')\n",
    "        s=0\n",
    "        t=str(len(img_filenames))\n",
    "        for img_filename in img_filenames:\n",
    "            key = img_filename[:-4]\n",
    "            if s % 50 is 0:\n",
    "                print(\"Testing the image with id: \" + str(key) + \"... \" + str(s+1) + \"/\" + t)\n",
    "            img = Image.open(os.path.join(test_dir, img_filename))\n",
    "            img = img.resize((IMAGE_DIMENSION, IMAGE_DIMENSION))\n",
    "\n",
    "            img_arr = np.expand_dims(np.array(img), 0)\n",
    "            out_sigmoid = model.predict(x=img_arr / 255.)\n",
    "\n",
    "            for i in range(len(out_sigmoid[0])):\n",
    "                for j in range(len(out_sigmoid[0][i])):\n",
    "                    if out_sigmoid[0][i][j][0] > treshold:\n",
    "                        out_sigmoid[0][i][j][0] = 1\n",
    "                    else:\n",
    "                        out_sigmoid[0][i][j][0] = 0\n",
    "\n",
    "            predicted_class = (np.reshape(out_sigmoid, (1, IMAGE_DIMENSION, IMAGE_DIMENSION)))[0]\n",
    "\n",
    "            target = np.array(Image.open(os.path.join(test_dir, img_filename)).resize((IMAGE_DIMENSION, IMAGE_DIMENSION)))\n",
    "            prediction_img = np.zeros([target.shape[0], target.shape[1], 1])\n",
    "            prediction_img[np.where(predicted_class == 0)] = 0\n",
    "            prediction_img[np.where(predicted_class == 1)] = 1\n",
    "            \n",
    "            f.write(key + ',' + str(rle_encode(prediction_img)) + ',' + str(IMAGE_DIMENSION) + ',' + str(IMAGE_DIMENSION) + '\\n')\n",
    "            s = s + 1     \n",
    "\n",
    "# ------------------------------------------------PARAMETERS DEFINITIONS (FOR EACH OF THE 4 STEPS)\n",
    "\n",
    "for xa in range(4):\n",
    "    IMAGE_DIMENSION = 256\n",
    "    \n",
    "    cwd = os.getcwd()\n",
    "\n",
    "    img_h = IMAGE_DIMENSION\n",
    "    img_w = IMAGE_DIMENSION\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    if xa is 0:\n",
    "        SEED = 0\n",
    "        tf.random.set_seed(SEED)  \n",
    "        random.seed = SEED\n",
    "        lr = 2e-5\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        validation_split = 0.05\n",
    "        bs = 64\n",
    "        IOU_index = 0.15\n",
    "        apply_data_augmentation = True\n",
    "        print(\"We are applying data augmentation!\")\n",
    "        epochs = 1\n",
    "        VALIDATION = True  \n",
    "        med_droput = 0.2\n",
    "    if xa is 1:\n",
    "        SEED = 1\n",
    "        tf.random.set_seed(SEED)  \n",
    "        random.seed = SEED\n",
    "        lr = 0.05\n",
    "        optimizer = tf.keras.optimizers.Adadelta(learning_rate=lr)\n",
    "        validation_split = 0.01\n",
    "        bs = 32\n",
    "        IOU_index = 0.15\n",
    "        apply_data_augmentation = False\n",
    "        print(\"We are not applying data augmentation!\")\n",
    "        epochs = 1\n",
    "        VALIDATION = True  \n",
    "        med_droput = 0.001\n",
    "    if xa is 2:\n",
    "        SEED = 2\n",
    "        tf.random.set_seed(SEED)  \n",
    "        random.seed = SEED\n",
    "        lr = 0.1\n",
    "        optimizer = tf.keras.optimizers.Adadelta(learning_rate=lr)\n",
    "        validation_split = 0.4\n",
    "        bs = 8\n",
    "        IOU_index = 0.5\n",
    "        apply_data_augmentation = True\n",
    "        print(\"We are applying data augmentation!\")\n",
    "        epochs = 1\n",
    "        VALIDATION = False\n",
    "    if xa is 3:\n",
    "        SEED = 3\n",
    "        tf.random.set_seed(SEED)  \n",
    "        random.seed = SEED\n",
    "        lr = 0.1\n",
    "        optimizer = tf.keras.optimizers.Adadelta(learning_rate=lr)\n",
    "        validation_split = 0.05\n",
    "        bs = 2\n",
    "        IOU_index = 0.5\n",
    "        apply_data_augmentation = False\n",
    "        print(\"We are not applying data augmentation!\")\n",
    "        epochs = 1\n",
    "        VALIDATION = True\n",
    "\n",
    "\n",
    "    print(\"\\n\\nWE ARE USING THIS PARAMETERS: lr=\" + str(lr) + \", optimzer=\" + str(optimizer) + \n",
    "          \", validation_split=\" + str(validation_split))\n",
    "    print(\"batch_size=\" + str(bs) + \", IOU_Index=\" + str(IOU_index))\n",
    "    \n",
    "# ------------------------------------------------DATA AUGMENTATION AND DATASETS GENERATORS DEFINITIONS\n",
    "\n",
    "    data_gen_args = dict(brightness_range=[0.7, 1.0],\n",
    "                         shear_range=0.2,\n",
    "                         zoom_range=[1.0, 1.6],\n",
    "                         horizontal_flip=True,\n",
    "                         vertical_flip=True,\n",
    "                         fill_mode='reflect',\n",
    "                         validation_split = validation_split,\n",
    "                         cval=0,\n",
    "                         rescale=1./255)\n",
    "\n",
    "    img_height = IMAGE_DIMENSION\n",
    "    img_width = IMAGE_DIMENSION\n",
    "    num_channels = 3\n",
    "    img_shape = (img_height, img_width, num_channels)\n",
    "    num_classes = 1\n",
    "\n",
    "    # ---------------------------DATASET\n",
    "\n",
    "    if apply_data_augmentation:\n",
    "        train_img_data_gen = ImageDataGenerator(**data_gen_args)\n",
    "        train_mask_data_gen = ImageDataGenerator(**data_gen_args)\n",
    "    else:\n",
    "        train_img_data_gen = ImageDataGenerator(validation_split = validation_split, rescale=1./255)\n",
    "        train_mask_data_gen = ImageDataGenerator(validation_split = validation_split, rescale=1./255)\n",
    "\n",
    "    valid_img_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "    valid_mask_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "    train_img_gen = train_img_data_gen.flow_from_directory(training_dir, target_size=(img_h, img_w), batch_size=bs, \n",
    "                                                            class_mode=None, shuffle=True, interpolation='bicubic',\n",
    "                                                            subset='training', seed=SEED)  \n",
    "    train_mask_gen = train_mask_data_gen.flow_from_directory(masks_dir, color_mode='grayscale', target_size=(img_h, img_w),\n",
    "                                                                batch_size=bs, class_mode=None, shuffle=True,\n",
    "                                                                interpolation='bicubic', subset='training', seed=SEED)\n",
    "    train_gen = zip(train_img_gen, train_mask_gen)\n",
    "\n",
    "    valid_img_gen = train_img_data_gen.flow_from_directory(training_dir, target_size=(img_h, img_w), batch_size=bs, \n",
    "                                                            class_mode=None, shuffle=True, interpolation='bicubic',\n",
    "                                                            subset='validation', seed=SEED)\n",
    "    valid_mask_gen = train_mask_data_gen.flow_from_directory(masks_dir, color_mode='grayscale', target_size=(img_h, img_w),\n",
    "                                                                batch_size=bs, class_mode=None, shuffle=True,\n",
    "                                                                interpolation='bicubic', subset='validation', seed=SEED)\n",
    "    valid_gen = zip(valid_img_gen, valid_mask_gen)\n",
    "\n",
    "    def prepare_target(x_, y_):\n",
    "        y_ = tf.cast(y_, tf.float32)\n",
    "        return x_, y_\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_generator(lambda: train_gen, output_types=(tf.float32, tf.float32),\n",
    "                                                    output_shapes=([None, img_h, img_w, 3], [None, img_h, img_w, 1]))\n",
    "    train_dataset = train_dataset.map(prepare_target)\n",
    "    train_dataset = train_dataset.repeat()\n",
    "\n",
    "    valid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen, output_types=(tf.float32, tf.float32),\n",
    "                                                    output_shapes=([None, img_h, img_w, 3], [None, img_h, img_w, 1]))\n",
    "    valid_dataset = valid_dataset.map(prepare_target)\n",
    "    valid_dataset = valid_dataset.repeat()\n",
    "\n",
    "    # ---------------------------MODEL DEFINITION\n",
    "\n",
    "    # ---------------------------------------ENCODER\n",
    "    inputs = tf.keras.layers.Input((IMAGE_DIMENSION, IMAGE_DIMENSION, 3))\n",
    "    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation=tf.keras.activations.elu,\n",
    "                                padding='same')(inputs)\n",
    "    c1 = tf.keras.layers.Dropout(0.05)(c1)\n",
    "    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation=tf.keras.activations.elu,\n",
    "                                padding='same')(c1)\n",
    "    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation=tf.keras.activations.elu,\n",
    "                                padding='same')(c1)\n",
    "    c1 = tf.keras.layers.Dropout(med_droput)(c1)                                \n",
    "    p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1) #--\n",
    "\n",
    "    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation=tf.keras.activations.elu,\n",
    "                                padding='same')(p1)\n",
    "    c2 = tf.keras.layers.Dropout(med_droput)(c2)\n",
    "    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation=tf.keras.activations.elu,\n",
    "                                padding='same')(c2)\n",
    "    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation=tf.keras.activations.elu,\n",
    "                                padding='same')(c2)     \n",
    "    c2 = tf.keras.layers.Dropout(med_droput)(c2)                           \n",
    "    p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2) #--\n",
    "\n",
    "    c3 = tf.keras.layers.Conv2D(64, (3, 3), activation=tf.keras.activations.elu,\n",
    "                                padding='same')(p2)\n",
    "    c3 = tf.keras.layers.Dropout(med_droput)(c3)\n",
    "    c3 = tf.keras.layers.Conv2D(64, (3, 3), activation=tf.keras.activations.elu,\n",
    "                                padding='same')(c3)\n",
    "    c3 = tf.keras.layers.Conv2D(64, (3, 3), activation=tf.keras.activations.elu,\n",
    "                                padding='same')(c3)\n",
    "    c3 = tf.keras.layers.Dropout(med_droput)(c3)\n",
    "    p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3) #--\n",
    "\n",
    "    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation=tf.keras.activations.elu,\n",
    "                                padding='same')(p3)\n",
    "    c4 = tf.keras.layers.Dropout(med_droput)(c4)\n",
    "    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation=tf.keras.activations.elu,\n",
    "                                padding='same')(c4)\n",
    "    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation=tf.keras.activations.elu,\n",
    "                                padding='same')(c4)\n",
    "    c4 = tf.keras.layers.Dropout(med_droput)(c4)\n",
    "    p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4) #--\n",
    "\n",
    "    c5 = tf.keras.layers.Conv2D(256, (3, 3), activation=tf.keras.activations.elu,\n",
    "                                padding='same')(p4)\n",
    "    c5 = tf.keras.layers.Dropout(med_droput)(c5)\n",
    "    c5 = tf.keras.layers.Conv2D(256, (3, 3), activation=tf.keras.activations.elu,\n",
    "                                padding='same')(c5) #--\n",
    "    c5 = tf.keras.layers.Conv2D(256, (3, 3), activation=tf.keras.activations.elu,\n",
    "                                padding='same')(c5)\n",
    "    c5 = tf.keras.layers.Dropout(med_droput)(c5)\n",
    "\n",
    "    # ----------------------------------------------------------Plateu\n",
    "\n",
    "    c5 = tf.keras.layers.Conv2D(512, (3, 3), activation=tf.keras.activations.elu,\n",
    "                                padding='same')(c5)\n",
    "    c5 = tf.keras.layers.Dropout(0.1)(c5)\n",
    "    c5 = tf.keras.layers.Conv2D(512, (3, 3), activation=tf.keras.activations.elu,\n",
    "                                padding='same')(c5)\n",
    "    c5 = tf.keras.layers.Dropout(0.1)(c5)\n",
    "    c5 = tf.keras.layers.Conv2D(512, (3, 3), activation=tf.keras.activations.elu,\n",
    "                                padding='same')(c5)\n",
    "    c5 = tf.keras.layers.Dropout(0.1)(c5)\n",
    "\n",
    "    # ----------------------------------------------------------DECODER\n",
    "\n",
    "    u6 = tf.keras.layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c5) #--\n",
    "    u6 = tf.keras.layers.concatenate([u6, c4])\n",
    "    c6 = tf.keras.layers.Conv2D(256, (3, 3), activation=tf.keras.activations.elu,\n",
    "                                padding='same')(u6)\n",
    "    c6 = tf.keras.layers.Dropout(med_droput)(c6)\n",
    "    c6 = tf.keras.layers.Conv2D(256, (3, 3), activation=tf.keras.activations.elu,\n",
    "                                padding='same')(c6)\n",
    "    c6 = tf.keras.layers.Conv2D(256, (3, 3), activation=tf.keras.activations.elu,\n",
    "                                padding='same')(c6)\n",
    "    c6 = tf.keras.layers.Dropout(med_droput)(c6)                            \n",
    "\n",
    "    u7 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c6) #--\n",
    "    u7 = tf.keras.layers.concatenate([u7, c3])\n",
    "    c7 = tf.keras.layers.Conv2D(128, (3, 3), activation=tf.keras.activations.elu,\n",
    "                                padding='same')(u7)\n",
    "    c7 = tf.keras.layers.Dropout(med_droput)(c7)\n",
    "    c7 = tf.keras.layers.Conv2D(128, (3, 3), activation=tf.keras.activations.elu,\n",
    "                                padding='same')(c7)\n",
    "    c7 = tf.keras.layers.Conv2D(128, (3, 3), activation=tf.keras.activations.elu,\n",
    "                                padding='same')(c7)\n",
    "    c7 = tf.keras.layers.Dropout(med_droput)(c7)\n",
    "\n",
    "    u8 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c7) #--\n",
    "    u8 = tf.keras.layers.concatenate([u8, c2])\n",
    "    c8 = tf.keras.layers.Conv2D(64, (3, 3), activation=tf.keras.activations.elu,\n",
    "                                padding='same')(u8)\n",
    "    c8 = tf.keras.layers.Dropout(med_droput)(c8)\n",
    "    c8 = tf.keras.layers.Conv2D(64, (3, 3), activation=tf.keras.activations.elu,\n",
    "                                padding='same')(c8)\n",
    "    c8 = tf.keras.layers.Conv2D(64, (3, 3), activation=tf.keras.activations.elu,\n",
    "                                padding='same')(c8)\n",
    "    c8 = tf.keras.layers.Dropout(med_droput)(c8)\n",
    "\n",
    "    u9 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c8) #--\n",
    "    u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n",
    "    c9 = tf.keras.layers.Conv2D(32, (3, 3), activation=tf.keras.activations.elu,\n",
    "                                padding='same')(u9)\n",
    "    c9 = tf.keras.layers.BatchNormalization()(c9)\n",
    "    c9 = tf.keras.layers.Dropout(0.1)(c9)\n",
    "    c9 = tf.keras.layers.Conv2D(32, (3, 3), activation=tf.keras.activations.elu,\n",
    "                                padding='same')(c9)\n",
    "    c9 = tf.keras.layers.Conv2D(32, (3, 3), activation=tf.keras.activations.elu,\n",
    "                                padding='same')(c9)\n",
    "    c9 = tf.keras.layers.Dropout(0.1)(c9)\n",
    "    c9 = tf.keras.layers.Conv2D(4, (3, 3), activation=tf.keras.activations.elu,\n",
    "                                padding='same')(c9)\n",
    "\n",
    "    outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9) #OUTPUT\n",
    "\n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "    \n",
    "    \n",
    "# ------------------------------------------------MODEL COMPILATION (DEFINITION OF LOSS AND METRIC)\n",
    "\n",
    "    def my_IoU(y_true, y_pred):\n",
    "        y_pred = tf.cast(y_pred > IOU_index, tf.float32)\n",
    "        intersection = tf.reduce_sum(y_true * y_pred)\n",
    "        union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
    "        return intersection / union\n",
    "\n",
    "    def jaccard_distance_loss(y_true, y_pred, smooth=100):\n",
    "        intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "        sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "        jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "        loss = (1 - jac) * smooth\n",
    "        return loss\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=jaccard_distance_loss, metrics=[my_IoU])\n",
    "    #model.summary()\n",
    "\n",
    "    LOAD_FROM_FILE = True\n",
    "    try:\n",
    "        if LOAD_FROM_FILE:\n",
    "            if os.path.exists('my_model_weights.h5'):\n",
    "                print(\"WEIGHTS ARE LOADED!!\")\n",
    "                model.load_weights('my_model_weights.h5')\n",
    "    except Exception as e:\n",
    "        print(\"WEIGHTS NOT LOADED!!\")\n",
    "        print(e)\n",
    "\n",
    "    TRAIN = True\n",
    "    if TRAIN:\n",
    "        #reduceLR = tf.keras.callbacks.ReduceLROnPlateau(monitor='accuracy', factor=0.1, patience=4, mode='auto')\n",
    "        #early = tf.keras.callbacks.EarlyStopping(monitor='accuracy', min_delta=1e-4, patience=7, mode='auto')\n",
    "\n",
    "        if VALIDATION:\n",
    "            model.fit(x=train_dataset, epochs=epochs, steps_per_epoch=len(train_img_gen), \n",
    "                      validation_data=valid_dataset, validation_steps=len(valid_img_gen))\n",
    "        else:\n",
    "            model.fit(x=train_dataset, epochs=epochs, steps_per_epoch=len(train_img_gen))\n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "    SAVE = True\n",
    "    if SAVE:\n",
    "        model.save_weights('my_model_weights2.h5')\n",
    "print()\n",
    "print(\"--------------\")\n",
    "TEST = True\n",
    "if TEST:\n",
    "    saveTestDict(test_dir, IMAGE_DIMENSION, 0.28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "raEGRq-xCY4P"
   },
   "source": [
    "## Using transfer learning\n",
    "Here instead we tried to use transfer learning in order to have final better performance from the model.\n",
    "***\n",
    "Also this time the final approach consists in 4 steps where the different parameters are changed. Further we used, for the downsampling part the **VGG** network, for the upsampling one the **pix2pix** network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mq3CIIhgCYfJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "We are applying data augmentation!\n",
      "\n",
      "\n",
      "WE ARE USING THIS PARAMETERS: lr=1.0, optimzer=<tensorflow.python.keras.optimizer_v2.adadelta.Adadelta object at 0x000001F748E02388>, validation_split=0.1\n",
      "batch_size=16, IOU_Index=0.5\n",
      "Found 6883 images belonging to 1 classes.\n",
      "Found 6883 images belonging to 1 classes.\n",
      "Found 764 images belonging to 1 classes.\n",
      "Found 764 images belonging to 1 classes.\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 13s 0us/step\n",
      "Train for 431 steps, validate for 48 steps\n",
      "Epoch 1/20\n",
      "  1/431 [..............................] - ETA: 59:02"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1dc63a1639fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    340\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mVALIDATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m             model.fit(x=train_dataset, epochs=epochs, steps_per_epoch=len(train_img_gen), \n\u001b[1;32m--> 342\u001b[1;33m                       validation_data=valid_dataset, validation_steps=len(valid_img_gen))\n\u001b[0m\u001b[0;32m    343\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_img_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FirstTest\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "!pip install -q git+https://github.com/tensorflow/examples.git\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from IPython.display import FileLink, FileLinks\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import os, sys, random, time, scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import regularizers\n",
    "from keras.layers import BatchNormalization as BatchNorm\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Reshape, Activation\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "\n",
    "get_ipython().run_line_magic('matplotlib', 'notebook')\n",
    "\n",
    "dataset_dir = 'Segmentation_Dataset'\n",
    "images_dir = os.path.join(dataset_dir, 'training')\n",
    "training_dir = os.path.join(images_dir, 'images')\n",
    "masks_dir = os.path.join(images_dir, 'masks')\n",
    "test_dir = os.path.join(dataset_dir, 'test/images/img')\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "    \n",
    "def rle_encode(img):\n",
    "    pixels = img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def saveTestDict(test_dir, IMAGE_DIMENSION, treshold):\n",
    "    img_filenames = next(os.walk(test_dir))[2]\n",
    "    \n",
    "    csv_fname = 'results_'\n",
    "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
    "\n",
    "    with open(csv_fname, 'w') as f:\n",
    "\n",
    "        f.write('ImageId,EncodedPixels,Width,Height\\n')\n",
    "        s=0\n",
    "        t=str(len(img_filenames))\n",
    "        for img_filename in img_filenames:\n",
    "            key = img_filename[:-4]\n",
    "            if s % 50 is 0:\n",
    "                print(\"Testing the image with id: \" + str(key) + \"... \" + str(s+1) + \"/\" + t)\n",
    "            img = Image.open(os.path.join(test_dir, img_filename))\n",
    "            img = img.resize((IMAGE_DIMENSION, IMAGE_DIMENSION))\n",
    "\n",
    "            img_arr = np.expand_dims(np.array(img), 0)\n",
    "            out_sigmoid = model.predict(x=img_arr / 255.)\n",
    "\n",
    "            for i in range(len(out_sigmoid[0])):\n",
    "                for j in range(len(out_sigmoid[0][i])):\n",
    "                    if out_sigmoid[0][i][j][0] > treshold:\n",
    "                        out_sigmoid[0][i][j][0] = 1\n",
    "                    else:\n",
    "                        out_sigmoid[0][i][j][0] = 0\n",
    "\n",
    "            predicted_class = (np.reshape(out_sigmoid, (1, IMAGE_DIMENSION, IMAGE_DIMENSION)))[0]\n",
    "\n",
    "            target = np.array(Image.open(os.path.join(test_dir, img_filename)).resize((IMAGE_DIMENSION, IMAGE_DIMENSION)))\n",
    "            prediction_img = np.zeros([target.shape[0], target.shape[1], 1])\n",
    "            prediction_img[np.where(predicted_class == 0)] = 0\n",
    "            prediction_img[np.where(predicted_class == 1)] = 1\n",
    "            \n",
    "            f.write(key + ',' + str(rle_encode(prediction_img)) + ',' + str(IMAGE_DIMENSION) + ',' + str(IMAGE_DIMENSION) + '\\n')\n",
    "            s = s + 1     \n",
    "\n",
    "# ---------------------------PARAMETERS\n",
    "\n",
    "for xa in range(4):\n",
    "    IMAGE_DIMENSION = 256\n",
    "    \n",
    "    cwd = os.getcwd()\n",
    "\n",
    "    img_h = IMAGE_DIMENSION\n",
    "    img_w = IMAGE_DIMENSION\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    if xa is 0:\n",
    "        SEED = 0\n",
    "        tf.random.set_seed(SEED)  \n",
    "        random.seed = SEED\n",
    "        lr = 1.0\n",
    "        optimizer = tf.keras.optimizers.Adadelta(learning_rate=lr)\n",
    "        validation_split = 0.1\n",
    "        bs = 16\n",
    "        IOU_index = 0.5\n",
    "        apply_data_augmentation = True\n",
    "        print(\"We are applying data augmentation!\")\n",
    "        epochs = 20\n",
    "        VALIDATION = True  \n",
    "        med_droput = 0.1\n",
    "    if xa is 1:\n",
    "        SEED = 1\n",
    "        tf.random.set_seed(SEED)  \n",
    "        random.seed = SEED\n",
    "        lr = 1.0\n",
    "        optimizer = tf.keras.optimizers.Adadelta(learning_rate=lr)\n",
    "        validation_split = 0.1\n",
    "        bs = 16\n",
    "        IOU_index = 0.5\n",
    "        apply_data_augmentation = False\n",
    "        print(\"We are not applying data augmentation!\")\n",
    "        epochs = 20\n",
    "        VALIDATION = True  \n",
    "        med_droput = 0.1\n",
    "    if xa is 2:\n",
    "        SEED = 2\n",
    "        tf.random.set_seed(SEED)  \n",
    "        random.seed = SEED\n",
    "        lr = 0.5\n",
    "        optimizer = tf.keras.optimizers.Adadelta(learning_rate=lr)\n",
    "        validation_split = 0.1\n",
    "        bs = 16\n",
    "        IOU_index = 0.5\n",
    "        apply_data_augmentation = True\n",
    "        print(\"We are applying data augmentation!\")\n",
    "        epochs = 20\n",
    "        VALIDATION = True  \n",
    "        med_droput = 0.1\n",
    "    if xa is 3:\n",
    "        SEED = 3\n",
    "        tf.random.set_seed(SEED)  \n",
    "        random.seed = SEED\n",
    "        lr = 0.5\n",
    "        optimizer = tf.keras.optimizers.Adadelta(learning_rate=lr)\n",
    "        validation_split = 0.1\n",
    "        bs = 16\n",
    "        IOU_index = 0.5\n",
    "        apply_data_augmentation = False\n",
    "        print(\"We are not applying data augmentation!\")\n",
    "        epochs = 20\n",
    "        VALIDATION = True  \n",
    "        med_droput = 0.1\n",
    "\n",
    "    print(\"\\n\\nWE ARE USING THIS PARAMETERS: lr=\" + str(lr) + \", optimzer=\" + \n",
    "          str(optimizer) + \", validation_split=\" + str(validation_split))\n",
    "    print(\"batch_size=\" + str(bs) + \", IOU_Index=\" + str(IOU_index))\n",
    "    \n",
    "    data_gen_args = dict(brightness_range=[0.7, 1.0],\n",
    "                         shear_range=0.2,\n",
    "                         zoom_range=[1.0, 1.6],\n",
    "                         horizontal_flip=True,\n",
    "                         vertical_flip=True,\n",
    "                         fill_mode='reflect',\n",
    "                         validation_split = validation_split,\n",
    "                         cval=0,\n",
    "                         rescale=1./255)\n",
    "\n",
    "    img_height = IMAGE_DIMENSION\n",
    "    img_width = IMAGE_DIMENSION\n",
    "    num_channels = 3\n",
    "    img_shape = (img_height, img_width, num_channels)\n",
    "    num_classes = 1\n",
    "\n",
    "    # ---------------------------DATASET\n",
    "\n",
    "    \n",
    "    \n",
    "    if apply_data_augmentation:\n",
    "        train_img_data_gen = ImageDataGenerator(**data_gen_args)\n",
    "        train_mask_data_gen = ImageDataGenerator(**data_gen_args)\n",
    "    else:\n",
    "        train_img_data_gen = ImageDataGenerator(validation_split = validation_split, rescale=1./255)\n",
    "        train_mask_data_gen = ImageDataGenerator(validation_split = validation_split, rescale=1./255)\n",
    "\n",
    "    valid_img_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "    valid_mask_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "    train_img_gen = train_img_data_gen.flow_from_directory(training_dir, target_size=(img_h, img_w), batch_size=bs, \n",
    "                                                            class_mode=None, shuffle=True, interpolation='bicubic',\n",
    "                                                            subset='training', seed=SEED)  \n",
    "    train_mask_gen = train_mask_data_gen.flow_from_directory(masks_dir, color_mode='grayscale', target_size=(img_h, img_w),\n",
    "                                                                batch_size=bs, class_mode=None, shuffle=True,\n",
    "                                                                interpolation='bicubic', subset='training', seed=SEED)\n",
    "    train_gen = zip(train_img_gen, train_mask_gen)\n",
    "\n",
    "    valid_img_gen = train_img_data_gen.flow_from_directory(training_dir, target_size=(img_h, img_w), batch_size=bs, \n",
    "                                                            class_mode=None, shuffle=True, interpolation='bicubic',\n",
    "                                                            subset='validation', seed=SEED)\n",
    "    valid_mask_gen = train_mask_data_gen.flow_from_directory(masks_dir, color_mode='grayscale', target_size=(img_h, img_w),\n",
    "                                                                batch_size=bs, class_mode=None, shuffle=True,\n",
    "                                                                interpolation='bicubic', subset='validation', seed=SEED)\n",
    "    valid_gen = zip(valid_img_gen, valid_mask_gen)\n",
    "\n",
    "    def prepare_target(x_, y_):\n",
    "        y_ = tf.cast(y_, tf.float32)\n",
    "        return x_, y_\n",
    "\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_generator(lambda: train_gen, output_types=(tf.float32, tf.float32),\n",
    "                                                    output_shapes=([None, img_h, img_w, 3], [None, img_h, img_w, 1]))\n",
    "    train_dataset = train_dataset.map(prepare_target)\n",
    "    train_dataset = train_dataset.repeat()\n",
    "\n",
    "    valid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen, output_types=(tf.float32, tf.float32),\n",
    "                                                    output_shapes=([None, img_h, img_w, 3], [None, img_h, img_w, 1]))\n",
    "    valid_dataset = valid_dataset.map(prepare_target)\n",
    "    valid_dataset = valid_dataset.repeat()\n",
    "\n",
    "    # ---------------------------MODEL\n",
    "\n",
    "    base_model = tf.keras.applications.VGG16(input_shape=[IMAGE_DIMENSION, IMAGE_DIMENSION, 3], include_top=False)\n",
    "    \n",
    "\n",
    "    # Use the activations of these layers\n",
    "    layer_names = [\n",
    "                   'block1_conv2',\n",
    "                   'block2_conv2',\n",
    "                   'block3_conv3',\n",
    "                   'block4_conv3',\n",
    "                   'block5_conv3'\n",
    "                   ]\n",
    "    layers = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "    # Create the feature extraction model\n",
    "    down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\n",
    "\n",
    "    down_stack.trainable = False\n",
    "\n",
    "    up_stack = [\n",
    "        pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
    "        pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
    "        pix2pix.upsample(128, 3)   # 32x32 -> 64x64\n",
    "    ]\n",
    "\n",
    "    \n",
    "    def unet_model(output_channels):\n",
    "        # This is the last layer of the model\n",
    "        last = tf.keras.layers.Conv2DTranspose(\n",
    "            output_channels, 3, strides=2,\n",
    "            padding='same', activation='sigmoid')  #64x64 -> 128x128\n",
    "\n",
    "        inputs = tf.keras.layers.Input(shape=[IMAGE_DIMENSION, IMAGE_DIMENSION, 3])\n",
    "        x = inputs\n",
    "\n",
    "        # Downsampling through the model\n",
    "        skips = down_stack(x)\n",
    "        x = skips[-1]\n",
    "        skips = reversed(skips[:-1])\n",
    "\n",
    "        # Upsampling and establishing the skip connections\n",
    "        for up, skip in zip(up_stack, skips):\n",
    "            x = up(x)\n",
    "            concat = tf.keras.layers.Concatenate()\n",
    "            x = concat([x, skip])\n",
    "\n",
    "        #x = tf.keras.layers.Dropout(med_droput)\n",
    "        x = last(x)\n",
    "\n",
    "        return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "    model = unet_model(1)\n",
    "\n",
    "    #model.summary()\n",
    "    #sys.exit()\n",
    "    # ---------------------------------------ENCODER\n",
    "    \n",
    "    \n",
    "\n",
    "    def my_IoU(y_true, y_pred):\n",
    "        y_pred = tf.cast(y_pred > IOU_index, tf.float32)\n",
    "        intersection = tf.reduce_sum(y_true * y_pred)\n",
    "        union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
    "        return intersection / union\n",
    "\n",
    "    def jaccard_distance_loss(y_true, y_pred, smooth=100):\n",
    "        intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "        sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "        jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "        loss = (1 - jac) * smooth\n",
    "        return loss\n",
    "\n",
    "    def iou_coef(y_true, y_pred, smooth=1):\n",
    "        intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "        union = K.sum(y_true,-1) + K.sum(y_pred,-1) - intersection\n",
    "        a = intersection + smooth\n",
    "        b = union + smooth\n",
    "        return a / b\n",
    "\n",
    "    def iou_coef_loss(y_true, y_pred):\n",
    "        return 1-iou_coef(y_true, y_pred)\n",
    "\n",
    "    \n",
    "    if xa is 0:\n",
    "        model.compile(optimizer=optimizer, loss=jaccard_distance_loss, metrics=[iou_coef])\n",
    "    if xa is 1:\n",
    "        model.compile(optimizer=optimizer, loss=jaccard_distance_loss, metrics=[iou_coef])\n",
    "    if xa is 2:\n",
    "        model.compile(optimizer=optimizer, loss=jaccard_distance_loss, metrics=[my_IoU])\n",
    "    if xa is 3: \n",
    "        model.compile(optimizer=optimizer, loss=jaccard_distance_loss, metrics=[my_IoU])\n",
    "\n",
    "    LOAD_FROM_FILE = True\n",
    "    try:\n",
    "        if LOAD_FROM_FILE:\n",
    "            if os.path.exists('my_model_weights2.h5'):\n",
    "                print(\"WEIGHTS FROM KAGGLE/WORKING ARE LOADED!!\")\n",
    "                model.load_weights('my_model_weights2.h5')\n",
    "            else:\n",
    "                if os.path.exists('/kaggle/input/lastws/my_model_weights.h5'):\n",
    "                    print(\"WEIGHTS FROM input database ARE LOADED!!\")\n",
    "                    model.load_weights('/kaggle/input/lastws/my_model_weights.h5')\n",
    "    except Exception as e:\n",
    "        print(\"WEIGHTS NOT LOADED!!\")\n",
    "        print(e)\n",
    "\n",
    "    TRAIN = True\n",
    "    if TRAIN:\n",
    "        #reduceLR = tf.keras.callbacks.ReduceLROnPlateau(monitor='accuracy', factor=0.1, patience=4, mode='auto')\n",
    "        #early = tf.keras.callbacks.EarlyStopping(monitor='accuracy', min_delta=1e-4, patience=7, mode='auto')\n",
    "\n",
    "        if VALIDATION:\n",
    "            model.fit(x=train_dataset, epochs=epochs, steps_per_epoch=len(train_img_gen), \n",
    "                      validation_data=valid_dataset, validation_steps=len(valid_img_gen))\n",
    "        else:\n",
    "            model.fit(x=train_dataset, epochs=epochs, steps_per_epoch=len(train_img_gen))\n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "    SAVE = True\n",
    "    if SAVE:\n",
    "        model.save_weights('my_model_weights2.h5')\n",
    "print()\n",
    "print(\"--------------\")\n",
    "TEST = False\n",
    "if TEST:\n",
    "    saveTestDict(test_dir, IMAGE_DIMENSION, 0.28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Meta Metaj - Segmentation",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
