{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Competition 3 - Visual Query Answering\nThe competition 3 is about both convolutional and recurrent neural networks. \n\nHere we have a huge dataset (with respect to the previous two competitions) composed by a reasonable number of images containing spheres, cones or cubes of different colors and materials and a ton of questions about the number, the colors or the materials of the objects.\n\nWe need to use Tensorflow 2.0 (with keras) in order to perform image classification on the images and to perform word understanding on the questions in the dataset; the final network will mix the two neural networks in order to understand the question, the relative image and the answer. \n\nThe final goal is to train the network in order to learn how to answer to new queries on new images containing the same kind of objects.\n\n---\n\nWe tried different approaches and we will present the 2 main final ones (the output of this notebook, however, will be not the same we had in the challenge for computer performance reasons).\n\n> Remember that this notebook ran on kaggle, if you download the dataset change the paths used in the code"},{"metadata":{},"cell_type":"markdown","source":"First of all, we import all the needed libraries."},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nimport tensorflow as tf\nimport numpy as np\nimport json\nimport cv2\nimport math \nfrom matplotlib import pyplot as plt\nfrom tensorflow.keras.utils import Sequence\nimport os\nfrom datetime import datetime\nimport random","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Definetely we can not load all the 60k images in one single shot, if we try to do it we get a memory error from tensorflow that will not have more free RAM to store them.\n\nThe solution is to define a class \"DataGenerator\" in order to load the needed images at each STEP in the train process. This class will be used by tensorflow to load the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataGenerator(Sequence):\n    \"\"\"Generates data for Keras\n    Sequence based data generator. Suitable for building data generator for training and prediction.\n    \"\"\"\n    def __init__(self, list_IDs, image_path, train_input_questions, max_length,\n                 to_fit=True, batch_size=16, dim=(100, 150),\n                 n_channels=3, n_classes=13, shuffle=True):\n        \"\"\"Initialization\n        :param list_IDs: list of all 'label' ids to use in the generator  \n        = ANSWERS to the questions!\n        \n        :param image_path: path to images location \n        = IMAGES path!\n        \n        :param to_fit: True to return X and y, False to return X only \n        = TRUE for val e train, FALSE for test\n        \n        :param batch_size: batch size at each iteration \n        \n        :param dim: tuple indicating image dimension\n        \n        :param n_channels: number of image channels \n        = 3 for RGB images\n        \n        :param n_classes: number of output masks \n        \n        :param shuffle: True to shuffle label indexes after every epoch \n        = always true!\n        \"\"\"\n        self.list_IDs = list_IDs\n        self.train_input_questions = train_input_questions\n        self.image_path = image_path\n        self.to_fit = to_fit\n        self.batch_size = batch_size\n        self.dim = dim\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.shuffle = shuffle\n        self.img_h = dim[0]\n        self.img_w = dim[1]\n        self.max_length = max_length\n        self.on_epoch_end()\n\n    def __len__(self):\n        \"\"\"Denotes the number of batches per epoch\n        :return: number of batches per epoch\n        \"\"\"\n        return int(np.floor(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        \"\"\"Generate one batch of data\n        :param index: index of the batch\n        :return: X and y when fitting. X only when predicting\n        \"\"\"\n        # Generate indexes of the batch\n        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n\n        # Find list of IDs\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n\n        # Generate data\n        X = self._generate_X(list_IDs_temp)\n\n        if self.to_fit:\n            y = self._generate_y(list_IDs_temp)\n            return X, y\n        else:\n            return X\n\n    def on_epoch_end(self):\n        \"\"\"Updates indexes after each epoch\n        \"\"\"\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def _generate_X(self, list_IDs_temp):\n        \"\"\"Generates data containing batch_size images\n        :param list_IDs_temp: list of label ids to load\n        :return: batch of images\n        \"\"\"\n        # Initialization\n        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n        X2 = np.empty((self.batch_size, self.max_length))\n\n        # Generate data\n        for i, ID in enumerate(list_IDs_temp):\n            # Store sample\n            X[i,] = self._load_image(self.image_path[ID], self.img_w, self.img_h)\n            X2[i,] = (self.train_input_questions[ID]).tolist()\n        ole = [X2, X]\n        \n        return ole\n\n    def _generate_y(self, list_IDs_temp):\n        \"\"\"Generates data containing batch_size masks\n        :param list_IDs_temp: list of label ids to load\n        :return: batch if masks\n        \"\"\"\n        y = np.empty((self.batch_size, 1), dtype=int)\n\n        # Generate data\n        for i, ID in enumerate(list_IDs_temp):\n            # Store sample\n            y[i] = self.list_IDs[ID]\n\n        return y\n\n    def _load_image(self, image_path, img_w, img_h):\n        \"\"\"Load grayscale image\n        :param image_path: path to image to load\n        :return: loaded image\n        \"\"\"\n        #img = cv2.imread(image_path)\n        #img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        #img = img / 255\n        if self.to_fit:\n            image = cv2.imread(\"/kaggle/input/ann-and-dl-vqa/dataset_vqa/train/\" + image_path)\n        else:\n            image = cv2.imread(\"/kaggle/input/ann-and-dl-vqa/dataset_vqa/test/\" + image_path)   \n        image = cv2.resize(image, (img_w, img_h))\n        image = image/ 255.\n        return image","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have to load the data to train and to test the model through the JSON files.\nThere is also the definition of 2 functions that extract the correct label from the answer element (\"no\" will be indicated by \"11\" in the json files)."},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('/kaggle/input/ann-and-dl-vqa/dataset_vqa/train_data.json', 'r') as f1:\n      data = json.load(f1)\nf1.close()\n\nwith open('/kaggle/input/ann-and-dl-vqa/dataset_vqa/test_data.json', 'r') as f2:\n      data_test = json.load(f2)\nf2.close()\n\n#--------------------------------------------------\ndef get_correct_label(answer):\n    return {\n        '0': 0,\n        '1': 1,\n        '10': 2,\n        '2': 3,\n        '3': 4,\n        '4': 5,\n        '5': 6,\n        '6': 7,\n        '7': 8,\n        '8': 9,\n        '9': 10,\n        'no': 11,\n        'yes': 12\n    }.get(answer)\n\ndef get_correct_class(answer):\n    return {\n        0: 0,\n        1: 1,\n        2: 10,\n        3: 2,\n        4: 3,\n        5: 4,\n        6: 5,\n        7: 6,\n        8: 7,\n        9: 8,\n        10: 9,\n        11: \"no\",\n        12: \"yes\"\n    }.get(answer)","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To generate the lists of questions, images and answers we need to save the length of the json files. Further we set the SEED, the BS, the NUM_CLASSES and the images size."},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 1234\nrandom.seed(SEED)\nnp.random.seed(SEED)\nimg_w = 320\nimg_h = 240\nnum_classes = 13\nbatch_size = 64","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here the validation split is defined (we choose 0.04 because the train set is very large). The definition of the different lists we need in order to define the DataGenerators and the tokenizer follow.\n\nWe decided also to add to each question the terms StartOfSentence and EndOfSentence at the begin and at the end of them."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_questions_number = len(data['questions'])\n#train_questions_number = 12000\ntest_questions_number = len(data_test['questions'])\n\ntrain_questions = []\ntrain_images = []\ntrain_answers = []\n\nvalid_questions = []\nvalid_images = []\nvalid_answers = []\n\ntest_questions = []\ntest_images = []\ntest_ids = []\n\nvalidation_split = 0.1\n\ndata.setdefault\n\n# Set boundaries in train and valid datasets\n\nVALID_EXAMPLES = math.trunc((train_questions_number*validation_split))\nTRAIN_QUESTIONS = train_questions_number - VALID_EXAMPLES\n\n# Read the data and extract the relative questions, images and answers for training\nfor i in range(TRAIN_QUESTIONS):\n    question = data[\"questions\"][i][\"question\"]\n    question = \"<sos> \" + question + \" <eos>\"\n    train_questions.append(question)\n    image_path = data[\"questions\"][i][\"image_filename\"]\n    #image = cv2.imread(\"/kaggle/input/ann-and-dl-vqa/dataset_vqa/train/\" + image_path)\n    #image = cv2.resize(image, (150, 100))\n    #image = image/ 255.\n    train_images.append(image_path)\n    answer = data[\"questions\"][i][\"answer\"]\n    answer = get_correct_label(answer)\n    train_answers.append(answer)\n\n# Read the data and extract the relative questions, images and answers for validation\nfor i in range(TRAIN_QUESTIONS, TRAIN_QUESTIONS + VALID_EXAMPLES):\n    question = data[\"questions\"][i][\"question\"]\n    question = \"<sos> \" + question + \" <eos>\"\n    valid_questions.append(question)\n    image_path = data[\"questions\"][i][\"image_filename\"]\n    #image = cv2.imread(\"/kaggle/input/ann-and-dl-vqa/dataset_vqa/train/\" + image_path)\n    #image = cv2.resize(image, (150, 100))\n    #image = image / 255.\n    valid_images.append(image_path)\n    answer = data[\"questions\"][i][\"answer\"]\n    answer = get_correct_label(answer)\n    valid_answers.append(answer)\n\n# Read the data_test and extract the relative questions and images\nfor i in range(test_questions_number):\n    question = data_test[\"questions\"][i][\"question\"]\n    question = \"<sos> \" + question + \" <eos>\"\n    test_questions.append(question)\n    image_path = data_test[\"questions\"][i][\"image_filename\"]\n    #image = cv2.imread(\"/kaggle/input/ann-and-dl-vqa/dataset_vqa/test/\" + image_path)\n    #image = cv2.resize(image, (150, 100))\n    #image = image / 255.\n    test_images.append(image_path)\n    test_id = data_test[\"questions\"][i][\"question_id\"]\n    test_ids.append(test_id)","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we have the part about the tokenizer tool. \n\nIt permits to transform a list of string in a matrix of integers (in order to give it in input to our keras model). The matrix will have dimensions A x B where A is the number of strings and B is the maximum number of words in the string lists.\n\nFor example if we have a list composed by \"Hi, my name is Ted\" and \"Hello Ted, my name is Mark\" the tokenizer assign an unique id to each word:\n\nHi: 1, my: 2, name: 3, is: 4, Ted: 5, Hello: 6, Mark: 7. After that it rebuilds the strings using the ids.\n\n\"1 2 3 4 5\" \"6 5 2 3 4 7\" ... it checks the maximum number of words (6 here) and builds the final matrix:\n\n[ [0 1 2 3 4 5], [6 5 2 3 4 7] ]"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer()\n\ntokenizer.fit_on_texts(train_questions)\n\nsequences = tokenizer.texts_to_sequences(train_questions)\nmax_length = max(len(sequence) for sequence in sequences)\ntrain_input_questions = pad_sequences(sequences, maxlen=max_length)\n\ntokenizer.fit_on_texts(valid_questions)\nsequences = tokenizer.texts_to_sequences(valid_questions)\nmaronn = max(len(sequence) for sequence in sequences)\nvalid_input_questions = pad_sequences(sequences, maxlen=max_length)\n\ntokenizer.fit_on_texts(test_questions)\nsequences = tokenizer.texts_to_sequences(test_questions)\ntest_input_questions = pad_sequences(sequences, maxlen=max_length)\n\nwords_number = len(tokenizer.word_index) + 1","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we define the generators! Train and validation generators don't need special shrewdnesses, the test one instead yes: for example we must set the to_fit=False in order to not try to extract any type of answer (we don't have them!)."},{"metadata":{"trusted":true},"cell_type":"code","source":"training_generator = DataGenerator(train_answers, train_images, \n                                   train_input_questions, max_length, \n                                   batch_size=batch_size, dim=(img_h, img_w))\nvalidation_generator = DataGenerator(valid_answers, valid_images, \n                                     valid_input_questions, max_length, \n                                     batch_size=batch_size, dim=(img_h, img_w))\ntest_generator = DataGenerator(test_ids, test_images, \n                               test_input_questions,  max_length, \n                               to_fit=False, batch_size=1, \n                               dim=(img_h, img_w), n_classes=num_classes, shuffle=False)","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's define our model! For understanding the images we have a convolutional network with transfer learning from VGG16 and some added dropout layers.\n\nFor understanding the questions we use a bidirection GRU Recurrent Network (seems better than LSTM from papers about them).\n\nFinally we concatenate the two pieces adding the final dense part (adding also here some droput levels)."},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = tf.keras.applications.InceptionResNetV2(input_shape=(img_h, img_w, 3), include_top=False, weights='imagenet')\nfor i in range(len(base_model.layers) - 40):\n    base_model.layers[i].trainable = False\n\nglobal_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n\nvision_model = tf.keras.models.Sequential()\nvision_model.add(tf.keras.layers.Dropout(0.2))\nvision_model.add(base_model)\nvision_model.add(global_average_layer)\nvision_model.add(tf.keras.layers.Dropout(0.5))\nvision_model.add(tf.keras.layers.Flatten())\n\nimage_input = tf.keras.layers.Input(shape=(img_h, img_w, 3))\nencoded_image = vision_model(image_input)\n\n# Define RNN for language input\nquestion_input = tf.keras.layers.Input(shape=[max_length])\nembedded_question = tf.keras.layers.Embedding(input_dim=words_number, output_dim=1024, input_length=max_length)(question_input)\nencoded_question = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(1024, dropout=0.4, recurrent_dropout=0.2))(embedded_question)\n\n# Combine CNN and RNN to create the final model\nmerged = tf.keras.layers.concatenate([encoded_question, encoded_image])\noutput = tf.keras.layers.Dense(1024, activation='relu')(merged)\noutput = tf.keras.layers.Dropout(0.5)(output)\noutput = tf.keras.layers.Dense(512, activation='relu')(output)\noutput = tf.keras.layers.Dropout(0.3)(output)\noutput = tf.keras.layers.Dense(256, activation='relu')(output)\noutput = tf.keras.layers.Dropout(0.3)(output)\noutput = tf.keras.layers.Dense(128, activation='relu')(output)\noutput = tf.keras.layers.Dropout(0.5)(output)\noutput = tf.keras.layers.Dense(num_classes, activation='softmax')(output)\nvqa_model = tf.keras.models.Model(inputs=[question_input, image_input], outputs=output)","execution_count":9,"outputs":[{"output_type":"stream","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n219062272/219055592 [==============================] - 3s 0us/step\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"We define the optimizer, the loss, the metric and we can fit the model!"},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.RMSprop()\nloss = tf.keras.losses.SparseCategoricalCrossentropy()\nmetrics = ['accuracy']\n\nvqa_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\nvqa_model.fit_generator(generator=training_generator, validation_data=validation_generator, epochs=1)","execution_count":14,"outputs":[{"output_type":"stream","text":"  42/3649 [..............................] - ETA: 58:00 - loss: 1.6888 - accuracy: 0.6217","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-7c8a91523c9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvqa_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mvqa_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1827\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1828\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1829\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1831\u001b[0m   @deprecation.deprecated(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{},"cell_type":"markdown","source":"We have now to predict the test questions and to create the csv file for kaggle submission. (Here the model is not really trained, for time issues)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_csv(results_dir='./'):\n\n    csv_fname = 'results_'\n    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n\n    with open(os.path.join('./', csv_fname), 'w') as f:\n\n        f.write('Id,Category\\n')\n\n        for i in range(len(pred)):\n            f.write(str(test_ids[i]) + ',' + str(np.argmax(pred[i])) + '\\n')","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = vqa_model.predict_generator(test_generator)\ncreate_csv()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}